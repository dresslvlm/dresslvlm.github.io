<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Dress">
  <meta name="keywords" content="multimodal chatbot rlaif rlhf alignment">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dress</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <!-- <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script> -->
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Dress: <span class="is-size-2"><span
                  class="is-size-1">I</span>nstructing
                <span class="is-size-1">L</span>arge <span class="is-size-1">V</span>ision-Language <span
                  class="is-size-1">M</span>odels to <span class="is-size-1">A</span>lign and <span
                  class="is-size-1">I</span>nteract with <span class="is-size-1">H</span>umans via
                <span class="is-size-1">N</span>atural <span class="is-size-1">L</span>anguage
                <span class="is-size-1">F</span>eedback
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://yangyi-chen.github.io/" style="color:#f68946;font-weight:normal;">Yangyi
                  Chen</a>,
              </span>
              <span class="author-block">
                <a href="https://ksikka.com/" style="color:#008AD7;font-weight:normal;">Karan Sikka</a>,
              </span>
              <span class="author-block">
                <a href="https://mcogswell.io" style="color:#F2A900;font-weight:normal;">Michael Cogswell</a>,
              </span>
              <span class="author-block">
                <a href="https://blender.cs.illinois.edu/hengji.html" style="color:#F2A900;font-weight:normal;">Heng
                  Ji</a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/ajaydivakaran/" style="color:#f68946;font-weight:normal;">Ajay
                  Divakaran</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> SRI
                International</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> UIUC</span>
            </div>




            <!-- <div class="column has-text-centered">
            <h3 class="title is-3 publication-title">Improved Baselines with Visual Instruction Fine-tuning</h3>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://hliu.cc/" style="color:#f68946;font-weight:normal;">Haotian Liu<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://chunyuan.li/" style="color:#008AD7;font-weight:normal;">Chunyuan Li<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://yuheng-li.github.io" style="color:#008AD7;font-weight:normal;">Yuheng Li</a>,
              </span>
              <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae
                  Lee</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of
                Wisconsin-Madison</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Microsoft Research</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2311.10081.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/YangyiYY/LVLM_NLF" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>



                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
          ðŸ”¥<span style="color: #ff3860">[NEW!]</span> LLaVA-1.5 achieves SoTA on 11 benchmarks, with just simple
          modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100
          node, and surpasses methods that use billion-scale data.
          <br><br>
          LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna
          for general-purpose visual and language understanding,
          achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new
          state-of-the-art accuracy on Science QA.
        </h4>
      </div>
    </div>
  </section> -->

  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop" id="gradio">
      <gradio-app src="https://llava.hliu.cc"></gradio-app>
    </div>
  </section>

  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present DRESS , a large vision language model (LVLM) that innovatively exploits Natural Language feed-
              back (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key
              limitations in the state-of-the-art LVLMs. First, prior LVLMs gener- ally rely only on the instruction
              finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they
              are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual
              instruction tuning data is generally structured in a multi- turn dialogue format, the connections and
              dependencies among consecutive conversational turns are weak. This re- duces the capacity for effective
              multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types:
              critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is
              used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for
              improve- ment and is adopted to improve the interaction ability of the LVLMsâ€“ which focuses on LVLMsâ€™
              ability to refine responses by incorporating feedback in multi-turn interactions. To address the
              non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our
              experi- mental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and
              harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions
              compared to SOTA LVMLs.
            </p>

          </div>
        </div>
      </div>

    </div>
  </section>





</body>

</html>